# 자연어처리 기반 인공지능

## NLP

- 문서 분류
- 키워드 추출
- 감정 분석
- 문서 요약
- 기계 번역
- 챗봇

### 자연어 처리란

- 일상적으로 사용하는 자연어는 컴퓨터가 바로 이해할 수 없음 -> 자연어처리필요
- 자연어 처리 (Natural Language Processing)
  - 사람이 이해하는 자연어를 컴퓨터가 이해할 수 있는 값으로 변환하는 과정
  - 자연어 이해(NLU, Natural Language Understanding)
  - 자연어 생성(NLg, Natural Language Generation)

#### 자연어 데이터

- Task에 따라 데이터 구성 다름
- 감정 Task
- 요약 Task
- 대화 Task
  ...

---

### 자연어 임베딩 과정

- 정수 인코딩
- 패딩
- One-hot Encoding

#### 정수 인코딩

- 컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 있음
- 텍스트를 숫자로 바꾸는 여러가지 기법들 존재
  - 첫 단계로 각 단어를 고유한 정수에 맵핑시키는 전처리 작업이 필요할 때가 있음
- 인덱스를 부여하는 방법은 여러가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, 보통은 단어 등장 빈도수를 기준으로 정렬한 뒤에 부여함

#### 패딩

- 자연어 처리를 하다 보면 각 문장(또는 문서)은 서로 길이가 다를 수 있음
- 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있음
  - 병렬 연산을 위해서 여러 문자으이 길이를 임의로 동일하게 맞춰주는 작업이 필요

#### One-hot Encoding

- 텍스트보다 숫자로 바꾸는 대표적인, 기본적인 방법
  1. 정수인코딩을 수행 -> 각 단어에 고유한 정수를 부여
  2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고 다른 단어의 인덱스의 위치에 0을 부여
- 한계점
  - 단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어남
    - 저장 공간 측면에서 매우 비효율적
  - 단어의 유사도를 표현하지 못함

### 워드 임베딩

- 단어를 벡터로 표현하는 방법
- 단어를 밀집 표현으로 변환
- 희소표현 (Sparse Representation)
- 밀집표현 (Dense Representation)

#### Word2Vec

- 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 할 수 있는 방법

---

## 시계열 모델

### 발전 과정

#### RNN/LSTM

- 텍스트를 읽는 방법이 이전의 데이터의 정보를 반영하는 것에 착안하여 만들어진 NN 모델
- 길이가 길면, 입력이 먼저 들어온 노드일수록 학습에 반영이 되지 않음

#### Seq-to-Seq

- Neural Machin Translation(NMT)의 시대를 열게된 모델
- RNN or LSTM을 두개 붙여서 Encoder, Decoder로 사용하는 모델
- 여전히 RNN/LSTM이 길이지면 문제가 생김

#### Transformer

- Seq-to-seq의 문제였던 입력이 먼저 들어오는 노드가 정보를 주지 못했던 것을 attention으로 해결
- seq-to-seq에 attention을 추가한 방법을 self-attention이라는 구조로 바꾸어 계산량을 급격하게 감소시킴
- 문장 단위의 embedding quality도 좋아짐

#### BERT

- 엄청난 양의 파라미터와 학습량이 필요하지만 Large Model의 경우 성능이 엄청남
- 특정 Task뿐만 아니라 언어에 대한 이해 자체가 목표인 대용량 모델의 등장으로 multi-lingual language model이 많이 생겨남

참고 : 엘리스
