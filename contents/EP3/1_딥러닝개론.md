# 딥러닝개론\_인공지능

### 배열, 행렬, 벡터

- 배열(array) : 컴퓨터에서 일반적으로 사용하는 개념으로 수를 포함하는 어떤 데이터의 묶음을 의미
- 벡터(vector) : 1차원으로 묶은 수를 부르며 행만 구성된 것을 행벡터, 열만 구성된 것을 열벡터라 부름
- 행렬(matrix) : 2차원으로 묶은 수를 의미, 우리가 알고 있는 수많은 행렬 계산식에 사용

### 행렬

- 숫자들을 직사각형 형태로 행과 열에 따라 나열한 것

```python
A = [[-2,5,6],[5,2,7]]
```

- 행렬을 다룰 때, 우리는 실수를 스칼라라고 부름
- 스칼라배란 실수와 행렬의 곱셈을 뜻함
- 스칼라배에서, 행렬의 각 요소는 주어진 스칼라에 곱해짐

- 전치행렬(transposed matrix) : 어떤 행렬의 행과 열을 서로 맞바꾼 행렬

## 인공지능

- 컴퓨터가 인간처럼 생각하고 학습하고 판단하여 스스로 행동 하도록 만드는 기술

유형

- 강인공지능
- 약인공지능
- 초인공지능

### 머신러닝 학습 방법

#### 지도 학습

- supervised learning
- 입력과 이에 대응하는 정답 데이터를 연관시키는 관계를 학습하는 방법
- 입력과 출력 쌍이 데이터로 주어지는 경우 그들 사이의 대응 관계를 학습하게 됨

#### 비지도 학습

- unsupervised learning
- 출력 없이 또는 출력 값을 알려주지 않고 주어진 입력만으로 스스로 모델을 구축하여 학습하는 방법
- 입력만 있고 출력 즉 레이블이 없는 경우에 적용하며 입력 사이의 규칙성 등을 스스로 찾아내는 것이 학습의 주요 목표

#### 준지도 학습

- 적은 입력과 매칭하는 정답 데이터 쌍 그리고 정답이 없는 대량의 데이터의 관계를 학습하는 방법
- 소량의 라벨 데이터에는 지도학습을 적용하고, 대용량의 라벨 없는 데이터에는 비지도 학습을 적용

#### 강화 학습

- 주어진 입력에 대응하는 행동을 취하는 시스템에 대해 보상이 주어지게 되며, 이러한 보상을 이용하여 학습하는 방법
- 지도 학습과 달리 주어진 입력에 대한 출력, 즉 정답 행동이 주어지지 않음
- 주요 응용 분야 : 로봇, 게임, 내비게이션 등

#논리게이트, #XOR

### 퍼셉트론

- 퍼셉트론은 1957년에 제안한 초기 형태의 인공 신경망
- 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘
- 퍼셉트론은 실제 뇌를 구성하는 신경 세포 뉴런의 동작과 유사
- 신경 세포 뉴런은 가지돌기에서 신호를 받아들이고 이 신호가 일정치 이상의 크기를 가지면 축삭 돌기를 통해서 신호를 전달

#### 단층 퍼셉트론

- 가중치를 갖는 층이 한 층이기 때문에 단층 퍼셉트론이라고 부름

#### 다층 퍼셉트론

- 단층 퍼셉트론은 입력층과 출력층만 존재하지만, 다층 퍼셉트론은 중간에 은닉층(hidden layer)라 불리는 층을 더 추가함
- 선형적으로만 풀던 게이트 문제를 여러 층을 추가함으로써 비선형적으로 풀 수 있게 됨
- 따라서 XOR 문제를 해결
- 은닉층은 2개일 수도, 수십, 수백개일 수도 있음
- 은닉층이 2개 이상인 신경망을 신층 신경망(Deep Neural Network, DNN)이라함

---

## 딥러닝

- 머신러닝의 여러 방법론 중 하나
- 인경신경망에 기반하여 컴퓨터에게 사람의 사고방식을 가르치는 방법
- 인경신경망 : 생물학의 신경망에서 영감을 얻은 학습 알고리즘, 사람의 신경 시스템을 모방함

- 논리 회로의 역할을 수행하는 퍼셉트론

  - 사람의 신경계 : 뉴런 -> 신경망 -> 지능
  - 딥러닝 : 퍼셉트론 -> 인공신경망 -> 인공지능

- 비 선형적인 문제 : 단층 퍼센트론은 입력층과 출력층만 존재
- 다층 퍼셉트론 : 단층 퍼셉트론을 여러 개 쌓은 것

  - 히든 층이 3층이상일 때 깊은 신경망 : Deep Learning

- 가중치 : 노드간의 연결 강도
- Loss Function : 예측값과 실제값간의 오차값
- Optimization : 오차값을 최소화하는 모델의 인자를 찾는 것

### 딥러닝 모델의 학습 방법

- 예측값과 실제값 간의 오차값을 최소화하기 위해 오차값을 최소화 하는 모델의 인자를 찾는 알고리즘을 적용

- Gradient Descent : 가장 기본적인 최적화 알고리즘

  - Gradient : 특정 가중치에서의 기울기
  - Learning rate : 학습률

- 딥러닝에서는 역전파(Backpropagation)을 통해 각 가중치들의 기울기를 구할 수 있음

- 역전파 : 나의 목표 target값과 실제 모델이 예측한 output 값이 얼마나 차이나는지 구한 후 오차값을 다시 뒤로 전파해가며 변수들을 갱신하는 알고리즘

---

## 텐서플로우

- 가장 많이 사용되고 빠른 성장율을 가진 프레임워크
- 유연하고, 효율적이며, 확장성 있는 딥러닝 프레임워크
- 대형 클러스터 컴퓨터부터 스마트폰까지 다양한 디바이스에서 동작

### 텐서

- Tensor = Multidimensional arrays = Data
- 딥러닝에서 텐서는 다차원 배열로 나타내는 데이터를 의미

### 딥러닝 모델 구현 순서

1. 데이터셋 준비하기
2. 딥러닝 모델 구축하기
3. 모델 학습시키기
4. 평가 및 예측하기

#### 데이터셋 준비하기

- Epoch : 한번의 epoch는 전체 데이터 셋에 대해 한 번 학습을 완료한 상태
- Batch : 나눠진 데이터 셋
- iteration : epoch를 나누어서 실행하는 횟수를 의미

#### 딥러닝 모델 구축하기

- Keras : 텐서플로우의 패키지로 제공되는 고수준 API, 딥러닝 모델을 간단하고 빠르게 구현 가능

---

## 딥러닝 모델 학습의 문제정

1. 학습 속도 문제
2. 기울기 소실 문제
3. 초기값 설정 문제
4. 과적합 문제

### 학습 속도 문제

- 데이터의 개수가 폭발적으로 중가하여 딥러닝 모델 학습 시 소유되는 시간도 함께 증가

- 발생원인 : 전체 학습 데이터 셋을 사용하여 손실 함수를 계산하기 대문에 계산량이 너무 많아짐
- 해결 방법 : 전체 데이터가 아닌 부분 데이터만 활용하여 손실 함수를 계산하자 -> SGD (Stochastic Gradient Descent)

#### SGD

- 전체 데이터 대신 일부 조그마한 데이터 모음인 미니 배치에 대해서만 손실 함수를 계산
- 다소 부정확할 수 있지만, 훨씬 계산 속도가 빠르기 때문에 같은 시간에 더 많은 step을 갈 수 있음

#Momentum , #AdaGrade, #RMSProp, #Adam

### 기울기 소실 문제

- 더 깊고 더 넓은 망을 학습시키는 과정에서 출력값과 멀어질 수록 학습이 잘 안되는 현상 발생

### 초기값 설정 문제

- 초기값 설정 방식에 따른 성능 차이가 매우 크게 발생

### 과적합 문제

- 학습 데이터에 모델이 과하게 최적화되어 테스트 데이터에 대한 모델 성능이 저하

#### 과적화 방지 기법

- 정규화(Regularization)
- 드롭아웃(Dropout)
- 배치 정규화(Batch Normalization)

참고 : 엘리스
